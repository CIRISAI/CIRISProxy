# CIRISProxy LiteLLM Configuration
# Model routing for credit-gated LLM access

model_list:
  # Llama 4 Maverick - Primary model for CIRIS Agent
  # Best for structured outputs / tool calling
  # Pricing: Together ~$0.50/$0.80, Groq $0.50/$0.77, OpenRouter $0.11/$0.34 per 1M tokens
  - model_name: "llama-4-maverick"
    litellm_params:
      model: "together_ai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
      api_key: "os.environ/TOGETHER_API_KEY"

  - model_name: "together/llama-4-maverick"
    litellm_params:
      model: "together_ai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
      api_key: "os.environ/TOGETHER_API_KEY"

  - model_name: "groq/llama-4-maverick"
    litellm_params:
      model: "groq/meta-llama/llama-4-maverick-17b-128e-instruct"
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: "openrouter/llama-4-maverick"
    litellm_params:
      model: "openrouter/meta-llama/llama-4-maverick"
      api_key: "os.environ/OPENROUTER_API_KEY"
      extra_body:
        provider:
          ignore: ["Friendli", "Google"]

  # GPT-OSS-20B - OpenAI's open-weight model (Apache 2.0)
  # NOTE: Weak at structured outputs / tool calling - kept for reference
  # Cheapest option: Together $0.05/$0.20 per 1M tokens
  - model_name: "gpt-oss-20b"
    litellm_params:
      model: "together_ai/openai/gpt-oss-20b"
      api_key: "os.environ/TOGETHER_API_KEY"
      max_tokens: 131072

  - model_name: "together/gpt-oss-20b"
    litellm_params:
      model: "together_ai/openai/gpt-oss-20b"
      api_key: "os.environ/TOGETHER_API_KEY"
      max_tokens: 131072

  - model_name: "groq/gpt-oss-20b"
    litellm_params:
      model: "groq/openai/gpt-oss-20b"
      api_key: "os.environ/GROQ_API_KEY"
      max_tokens: 131072

  - model_name: "openrouter/gpt-oss-20b"
    litellm_params:
      model: "openrouter/openai/gpt-oss-20b"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 131072
      extra_body:
        provider:
          ignore: ["Friendli", "Google"]

  # Groq Models (fast, cost-effective)
  - model_name: "groq/llama-3.3-70b"
    litellm_params:
      model: "groq/llama-3.3-70b-versatile"
      api_key: "os.environ/GROQ_API_KEY"

  - model_name: "groq/llama-3.1-8b"
    litellm_params:
      model: "groq/llama-3.1-8b-instant"
      api_key: "os.environ/GROQ_API_KEY"

  # Together AI Models
  - model_name: "together/llama-3.1-70b"
    litellm_params:
      model: "together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
      api_key: "os.environ/TOGETHER_API_KEY"

  - model_name: "together/llama-3.1-8b"
    litellm_params:
      model: "together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
      api_key: "os.environ/TOGETHER_API_KEY"

  - model_name: "together/qwen-2.5-72b"
    litellm_params:
      model: "together_ai/Qwen/Qwen2.5-72B-Instruct-Turbo"
      api_key: "os.environ/TOGETHER_API_KEY"

  # OpenAI (optional fallback)
  - model_name: "openai/gpt-4o-mini"
    litellm_params:
      model: "gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"

  - model_name: "openai/gpt-4o"
    litellm_params:
      model: "gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"

  # Aliases for simpler client usage
  # Default model for CIRIS Agent - Llama 4 Maverick via OpenRouter (cheapest)
  # Pricing: OpenRouter $0.11/$0.34 per 1M tokens
  - model_name: "default"
    litellm_params:
      model: "openrouter/meta-llama/llama-4-maverick"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 16384  # Allow long outputs for agent responses
      extra_body:
        provider:
          ignore: ["Friendli", "Google"]

  - model_name: "fast"
    litellm_params:
      model: "groq/meta-llama/llama-4-maverick-17b-128e-instruct"
      api_key: "os.environ/GROQ_API_KEY"
      max_tokens: 16384  # Allow long outputs for agent responses

litellm_settings:
  # Custom callback for CIRISBilling integration
  # Format: "filename.instance_name" where the file is in the working directory
  callbacks:
    - billing_callback.billing_callback_instance
  success_callback:
    - billing_callback.billing_callback_instance
  failure_callback:
    - billing_callback.billing_callback_instance

  # Drop unsupported parameters instead of erroring
  drop_params: true

  # Request timeout (seconds)
  request_timeout: 120

  # Set verbose for debugging (disable in production)
  set_verbose: false

router_settings:
  # Routing strategy for model groups with multiple deployments
  routing_strategy: "simple-shuffle"

  # Retry settings
  num_retries: 2
  retry_after: 1

  # Error-specific retry policies
  # On retry, LiteLLM picks a different deployment from the pool
  retry_policy:
    TimeoutErrorRetries: 1
    RateLimitErrorRetries: 2
    InternalServerErrorRetries: 1

  # Cooldown triggers - provider excluded after N failures per minute
  allowed_fails_policy:
    TimeoutErrorAllowedFails: 3
    RateLimitErrorAllowedFails: 5
    InternalServerErrorAllowedFails: 3

  # Seconds to exclude a failing provider from routing
  cooldown_time: 60

  # Fallback chain - Llama 4 Maverick across 3 providers
  # Pricing: OpenRouter $0.11/$0.34, Groq $0.50/$0.77, Together ~$0.50/$0.80 per 1M tokens
  fallbacks:
    # Only fallback to other Llama 4 Maverick providers - never degrade to older models
    - "default": ["groq/llama-4-maverick", "together/llama-4-maverick"]
    - "llama-4-maverick": ["groq/llama-4-maverick", "together/llama-4-maverick"]
    - "together/llama-4-maverick": ["groq/llama-4-maverick", "openrouter/llama-4-maverick"]
    - "groq/llama-4-maverick": ["openrouter/llama-4-maverick", "together/llama-4-maverick"]
    - "openrouter/llama-4-maverick": ["groq/llama-4-maverick", "together/llama-4-maverick"]
    - "groq/llama-3.3-70b": ["together/llama-3.1-70b", "openai/gpt-4o-mini"]

general_settings:
  # Master key for admin operations (not for client auth)
  master_key: "os.environ/LITELLM_MASTER_KEY"

  # Custom auth function - validates Google ID tokens
  # See: https://docs.litellm.ai/docs/proxy/custom_auth
  custom_auth: "custom_auth.user_api_key_auth"

  # Disable the built-in database (we use CIRISBilling for billing)
  database_url: null

  # Allow all origins for mobile clients
  allowed_ips: null

  # Custom endpoints (/v1/status, /v1/search) are added via server.py wrapper
  # See server.py for implementation - these routes are added directly to FastAPI
